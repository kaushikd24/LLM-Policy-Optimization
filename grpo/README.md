# GRPO: Group Relative Policy Optimization

##Overview

**Group Relative Policy Optimization (GRPO)** is a lightweight and efficient reinforcement learning (RL) algorithm designed to fine-tune large language models (LLMs) *without* the computational overhead of a critic. It's particularly suited for tasks like instruction tuning and mathematical reasoning, where reward signal comparison across groups of outputs is more informative than scalar-valued feedback.

This repo implements the GRPO algorithm with HuggingFace Transformers + LoRA (via PEFT), adapted for autoregressive language modeling tasks like math instruction tuning, preference learning, and beyond.

> üí° GRPO was originally introduced in [DeepSeekMath](https://arxiv.org/abs/2402.03300), where it helped outperform all open-source 7B‚Äì70B models on the MATH benchmark ‚Äî with fewer parameters and less compute than PPO.

---

## üöÄ Key Features

* üî• **KL-based loss over relative sequence scores** ‚Äî no critic network needed.
* üß© **LoRA-compatible** via HuggingFace PEFT.
* üõ†Ô∏è Drop-in support for any `AutoModelForCausalLM`-style model (e.g., GPT-2, LLaMA).
* üß™ Built-in dataset support for GRPO-style JSON formats.
* üßÆ Optimized for training efficiency using `Trainer`.

---

## üóÇÔ∏è Project Structure

```bash
grpo/
‚îú‚îÄ‚îÄ grpo_trainer.py        # Custom HuggingFace Trainer with GRPO loss
‚îú‚îÄ‚îÄ train_grpo.py          # Training script (uses YAML config)
‚îú‚îÄ‚îÄ dataset_utils.py       # Dataset class to load and batch GRPO JSON data
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ grpo_config.yaml   # Example training config
‚îú‚îÄ‚îÄ models/                # (Optional) Pretrained or LoRA model dirs
‚îî‚îÄ‚îÄ data/                  # Your dataset (JSON format with scores)
```

---

## üß† How GRPO Works (TL;DR)

Instead of optimizing a scalar reward like PPO does with a critic, **GRPO**:

1. Samples multiple completions per prompt.
2. Computes log-probabilities over these sequences.
3. Uses human or model-provided **group scores** to compute a soft target distribution.
4. Applies a **KL divergence** loss between the model's sequence probabilities and the score-based target.

This avoids the need for a value network and is more stable under sparse rewards.

---

## üì¶ Setup

Install dependencies:

```bash
pip install -r requirements.txt
```

You'll need:

* `transformers`
* `peft`
* `datasets`
* `torch`
* `PyYAML`
* (Optional) `accelerate` if you plan to scale

---

## üß™ Example: Running GRPO Training

Configure your settings in `configs/grpo_config.yaml`. Then run:

```bash
python grpo/train_grpo.py
```

Your config YAML should look like this:

```yaml
model_path: gpt2
lora_path: path/to/lora/checkpoint
dataset_path: data/grpo_dataset.json
output_dir: grpo_outputs/
max_seq_length: 512
batch_size: 8
num_epochs: 3
logging_dir: logs/
```

---

## üß¨ Example Dataset Format

The dataset should be a JSONL file where each entry contains multiple completions per prompt with associated scores:

```json
{
  "prompt": "Prove that the square root of 2 is irrational.",
  "completions": [
    {"text": "Assume sqrt(2) is rational...", "score": 3.2},
    {"text": "Using Pythagoras theorem...", "score": 1.8},
    ...
  ]
}
```

Each group is tokenized into a batch of shape `(B, K, T)` ‚Äî Batch size √ó Completions √ó Tokens ‚Äî for GRPO loss.

---

## üß† The GRPO Loss

```python
KL(œÄ_policy || œÄ_target), where
œÄ_policy = softmax(model log probs)
œÄ_target = softmax(human/model scores)
```

We apply this KL loss over sequence-level log-probs:

```python
loss = KLDiv(log_p, log_q)
```

Where `log_q` is detached to avoid backprop through the reward function.

---

## üìà Logging + Saving

* Logs every 20 steps (`logging_steps: 20`)
* Saves checkpoints every 500 steps
* Keeps the latest 2 checkpoints

Modify `TrainingArguments` in `train_grpo.py` if needed.

---

## üõ†Ô∏è Notes

* Make sure your tokenizer has a valid `pad_token_id` (we default to EOS).
* LoRA weights must be made trainable (`is_trainable=True`).
* Mixed-precision (fp16/bf16) training is easy to add via `TrainingArguments`.

---

## üìò Paper

> üìÑ *Group Relative Policy Optimization (GRPO)* was proposed in
> [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)
> (Zhihong Shao et al., 2024)

---

## üëÄ Related Work

* üßæ [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290)
* üß† [PPO (Proximal Policy Optimization)](https://arxiv.org/abs/1707.06347)
* ü§ñ [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)
* üèóÔ∏è [PEFT: Parameter-Efficient Fine-Tuning](https://huggingface.co/docs/peft)

---

## Files
- `train_grpo.py`: Main training script for GRPO.
- `grpo_trainer.py`: Custom trainer class for GRPO.
- `dataset_utils.py`: Utilities for preprocessing and loading datasets.
- `eval.py`: Evaluation utilities for assessing the performance of the GRPO model.

## Getting Started
To get started with training a model using GRPO, you can run the `train_grpo.py` script. Make sure to configure your environment and datasets appropriately.

## Requirements
Ensure all dependencies are installed as listed in the root `requirements.txt` file.

## License
This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments
- Thanks to the contributors and the open-source community for their support and resources.

# GRPO Training (LoRA + GPT-2)

This directory contains the GRPO training pipeline using HuggingFace + PEFT + LoRA.

## Run

```bash
python grpo/train_grpo.py --config configs/grpo_config.yaml
